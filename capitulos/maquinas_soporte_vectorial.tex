%!TEX root = ../notas_de_clase.tex
\section{Support Vector Machines}

Una de las desventajas de los clasificadores lineales vistos en el Capítulo \ref{cap:clasificacion} es la falta de atención a la distancia entre (en el caso binario) el elemento de clase 1 más cercano a la clase 2 y viceversa. Esta distancia es relevante pues nos da un sentido de generalización, es decir,  los elementos de la clase 1  \emph{más parecidos} a los de la clase 2 no están justo en el borde de las clases, sino que existe una zona donde podrían haber nuevas muestras en torno a datos existentes de clase, e.g., 1 y aún estar dentro de la región de clase 1. El tratamiento de este concepto (o la falta de él) es claro en los métodos anteriores: en el perceptrón, todas las soluciones que dividen las muestras clases 1 y 2 son ``igual de buenas'', es decir, el percpetrón es insensible al margen descrito arriba. Por otro lado, los métodos que funcionan mediante gradiente descendente, este margen se maximiza \emph{indirectamente}, lo cual conlleva a inestabilidades como en el caso en donde el parámetro de la regresión logística diverge. 


Para resolver el problema descrito anteriormente consideraremos las \textbf{máquinas de soporte vectorial} (SVM, por sus siglas en inglés). Como veremos en este capítulo, además de resolver el problema de clasificación binaria y lineal, la solución de SVM puede ser ocupado en muchas más situaciones, por esta razón, dedicamos un capítulo completo a este método en vez de verlo como un método de clasificación más. 

\subsection{Idea general}

Consideremos un problema de clasificación donde las clases son linealmente separables. Como podemos ver en la Fig.~\ref{fig:maxim_marg} (izquierda), en este caso existen diferentes hiperplanos (clasificadores lineales) que separan los datos de ambas clases de forma correcta. Cada una de estas soluciones define un modelo, donde dado un nuevo dato $x^*$, podemos evaluar a qué clase pertenece. Evidentemente, salvo la región cercana al límite de las clases, la asignación de clase usando cada uno de los clasificadores mostrados en dicha figura son similares. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/cap5_max_margen.pdf}
    \caption{Modelo de máximo margen entre los datos}
    \label{fig:maxim_marg} 
\end{figure}

Aquí entonces aflora la siguiente pregunta, como todas los clasificadores en esta figura separan de forma correcta los datos disponibles, ¿cuál de ellos debemos elegir? Podemos responder esta pregunta mediante la elección de un \emph{clasificador de máximo margen}, es decir, el clasificador que nos entregue la máxima separación entre los datos y las regiones definidas como correspondiente a cada clase. Una ilustración del clasificador de máximo margen se presenta en la Fig.~\ref{fig:maxim_marg} (derecha). En donde se ve que encontrar este clasificador es equivalente a encontrar una \emph{cinta} de ancho máximo que separe los datos de ambas clases.


El argumento de ocupar dicho criterio es la búsqueda de buenas propiedades de generalización. Intuitivamente, esta propiedad se obtiene debido a que asumiendo que los datos generados en cada clase  provienen de una distribución latente, es de esperar que si se obtienen nuevos datos desde la misma distribución, éstos estén cerca de los datos observados inicialmente. De este forma, con el máximo margen se pretende maximizar la probabilidad de que los nuevos datos de clase 1 (cf. 2) sean bien clasificados también.

Una propiedad clave del clasificador de máximo margen es que éste queda definido únicamente por algunos datos, ver Fig.~\ref{fig:maxim_marg} (derecha). Esto inmediatamente resuelve el problema de desbalances de clase o de que las clases tengan formas distintas (problema mencionado en clasificación con mínimos cuadrado o con el discriminante de Fisher). Es decir, la solución de máximo margen se mantiene si agregamos datos (de cualquier clase) que están fuera del margen. Estos datos (o vectores, pues recordemos que $x\in\R^M$) que definen el margen los llamaremos vectores de soporte (\emph{support vectors}), los que le entregan el nombre al método. 

Una caricatura para ilustrar el concepto de vector soporte se muestra en la Fig.~\ref{fig:manzanas}. Como los vectores soporte corresponden al \emph{elemento de la clase 1 más cercano a la clase 2} (y viceversa), en el caso de clasificar manzanas y naranjas, éstos equivalen a ``la manzana más naranjezca'' y la ``naranja más manzanezca''. 


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.3]{img/img5.png}
    \caption{Los vectores de soporte son la manzana más naranjezca y la naranja más manzanezca}
    \label{fig:manzanas}
\end{figure}


Finalmente, una diferencia fundamental entre SVM y otros métodos que aprenden incorporando  todos los datos para formar la respuesta (e.g., naive Bayes, regresión lineal, regresión logística). Por ejemplo, en base a la caricatura de la Fig.~\ref{fig:manzanas}, estos algoritmos calcularía lo lo que en promedio podría ser una manzana, mientras que SVM solo busca la manzana más naranjezca y la naranja mas manzanezca. Posteriormente, solo con estos elementos, SVM define el clasificador. 



\subsection{Formulación del problema}

Denotemos un conjunto de entrenamiento $\{x_i\}_{1,...,N}$ de clases linealmente separable y recordemos que un hiperplano de separación está definido mediante
\begin{equation}
    \{x\in \R^n | w\cdot x + b = 0 \}.\label{ec:hiperplano_svm}
\end{equation}

Donde $w\in \R^n$ es el vector perpendicular al hiperplano y $b\in \R$ es el \emph{offset}. El problema de clasificación consiste en encontrar dichos parámetros de acuerdo a algún criterio, en este caso es el criterio del máximo margen. Notemos en primer lugar que este problema no tiene solución única (pues si $w$ es solución entonces $\lambda w$, $\lambda>0$ también lo es). Para evitar esta \emph{invarianza} con tal de que el problema tenga solución única, podemos imponer una restricción sobre los bordes del margen. Denotando dos vectores soporte (uno para cada clase) mediante $x_{+}$ y $x_{-}$, imponemos que 
\begin{align}
 	w\cdot x_{+} + b &= 1 \label{ec:borde_svm1}\\
 	w\cdot x_{-} + b &=  -1,\label{ec:borde_svm2}
 \end{align}
 donde si bien puede haber más vectores soporte, es suficiente aplicar la restricción sobre solo un de cada clase. Observemos que si bien aún no sabemos cuales son los vectores de soporte, podemos imponer esta restricción de todas formas, las que se van a traducir en las restricciones del problema de optimización que definirá la solución del problema de clasificación. 

Notemos que las ecs.~\eqref{ec:hiperplano_svm},\eqref{ec:borde_svm1} y \eqref{ec:borde_svm2} definen tres hiperplanos paralelos, pues todos tienen el mismo parámetro $w$. La Fig.~\ref{fig:clasif_margen} ilustra las muestras de cada clase junto con estos tres hiperplanos, donde además se muestra el vector unitario perpendicular a (todos) estos hiperplanos $\frac{w}{||w||}$. El ancho del \emph{margen} $m$, es decir, la distancia entre la región de decisión y cualquiera de las clases, es la mitad de la diferencia entre ambos vectores de soporte, proyectada en la dirección del vector unitario mencionado, es decir, 

\begin{equation}
 	m = \frac{1}{2||w||}[ w \cdot (x_{+} - x_{-})]
\end{equation} 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{img/cap5_max_margen2.pdf}
    \caption{Clasificación de máximo margen. Clases denotadas por puntos azules y cruces rojas, región de decisión en gris sólido y bordes del margen en línea punteada. La línea naranja representa la  el vector diferencia $(x_+ - x_{-})$ y la flecha negra el vector unitario $\frac{w}{||w||}$.}
    \label{fig:clasif_margen}
\end{figure}

Veamos que el ancho del margen $m$ no depende explícitamente de los vectores soporte al incorporar las restricciones en las ecs.~\eqref{ec:borde_svm1}-\eqref{ec:borde_svm2}:

\begin{align}
    m &= \frac{1}{2||w||} [ (w\cdot x_{+}) - (w\cdot x_{-})]\nonumber\\
    &= \frac{1}{2||w||} [1 - (-1)]\nonumber\\
    &= \frac{1}{||w||}.\label{eq:margen}
\end{align}

En base a la expresión anterior para el ancho del margen, podemos formular el problema de clasificación de máximo margen. Consideramos la siguiente codificación para las clases:
\begin{alignat}{3}
 	y&=1 &&\Leftrightarrow w\cdot x_i + b \geq1 \label{eq:codif_svm1}\\
 	y_i &=-1 &&\Leftrightarrow w\cdot x_i + b \leq -1.\label{eq:codif_svm2}
 \end{alignat}
Con lo que la búsqueda de máximo margen puede ser expresada como el siguiente problema de optimización:
\begin{equation*}
\begin{aligned}
& \underset{w,b}{\text{max}}
& & \frac{1}{||w||}\\
& \text{s.a}
& & y_i (w\cdot x_i +b) \geq 1, \; i = 1, \ldots, N.
\end{aligned}
\end{equation*}
Lo cual simplemente quiere decir que se está maximizando la expresión del margen en la ec.~\eqref{eq:margen}, sujeto a que todas las muestras estén bien clasificadas. 

Para evitar problemas de diferenciablidad del recíproco de la raíz cuadrada, sobretodo cuando $w$ es cercano a 0, consideraremos la siguiente formulación equivalente del problema anterior:
\begin{equation*}
\begin{aligned}
& \underset{w,b}{\text{min}}
& & \frac{1}{2}||w||^2\\
& \text{s.a}
& & y_i (w\cdot x_i +b) \geq 1, \; i = 1, \ldots, N.
\end{aligned}
\end{equation*}

Este problema de optimización con restricciones puede ser resuelto mediante el método de Lagrange, donde resolvemos el \emph{problema dual}, el cual tiene una mejor estructura más ``amigable'' para resolver. Además, luego veremos que la resolución mediante este método es fundamental para al extensión no-lineal de SVM.

El lagrangiano del problema anterior está dado por:
\begin{equation}
    L(w,b,\alpha) = \frac{1}{2}||w||^2 - \sum\limits_{i=1}^{N} \alpha_i (y_i (w\cdot x_i +b) -1),\label{eq:lagrangiaano_svm}
\end{equation}
donde hemos denotado $\alpha = [\alpha_1,\ldots,\alpha_N]$ los multiplicadores de Lagrange. Las condiciones de primer primer orden sobre este problema dual nos dicen que
\begin{alignat}{3}
    \dfrac{\partial L}{\partial w} &= 0 \Rightarrow w &&= \sum\limits_{i=1}^{N} \alpha_i y_i x_i \label{eq:lag_svm_1}\\
    \dfrac{\partial L}{\partial b} &= 0 \Rightarrow 0 &&=\sum\limits_{i=1}^{N} \alpha_i y_i.  \label{eq:lag_svm_2}
\end{alignat}


Reemplazando entonces \eqref{eq:lag_svm_1} y \eqref{eq:lag_svm_2} en $L(w,b, \alpha)$ de la ec.~\eqref{eq:lagrangiaano_svm},  e imponiendo holgura complementaria, el nuevo problema de optimización (ahora de maximización por ser dual) está dado por: 
\begin{equation*}
\begin{aligned}
& \underset{\alpha}{\text{max}}
& & \sum\limits_{i=1}^{N}\alpha_i - \frac{1}{2} \sum\limits_{i,j=1}^{N} \alpha_i \alpha_j y_i y_j \langle x_i, x_j\rangle\\
& \text{s.a}
& & \sum\limits_{i=1}^{N} \alpha_i y_i= 0 \\
& &  &\alpha_i \geq 0
\end{aligned}
\end{equation*}
Este problema es del tipo QP (\emph{quadratic programming}), para el cual existen variados métodos para resolverlo de manera óptima y eficiente. 

Una vez resuelto la formulación dual (i.e., encontrar los valores óptimos para $\alpha$), la predicción de un nuevo punto $x^*$ es de la forma 
\begin{equation}
 	\hat{y}(x^*)= \text{sgn}\left(\left[\sum\limits_{i=1}^{N} \alpha_i y_i \langle x_i, x^*\rangle\right] - b\right),
 \end{equation}
 lo cual es consecuencia de incorporar la ec.~\eqref{eq:lag_svm_1} en la codificación de las ecs.~\eqref{eq:codif_svm1}-\eqref{eq:codif_svm2}.

Entonces, como por holgura complementaria tenemos que si $x_i$ \textbf{no está en el margen}, entonces $\alpha_i = 0$ y consecuentemente, $x_i$ no aporta en la predicción $\hat{y}$. Esta propiedad es la que mencionábamos al principio: la predicción de clase solo depende de los vectores de soporte, i.e., los que están en el margen. Esto ayuda a resolver el problema de optimización de manera más rápida, ya que en realidad solo algunas variables duales $\alpha_i$ serán no nulas (las correspondiente a los vectores que están en el borde del margen). Normalmente se ocupan heurísticas para encontrar y descartar rápidamente que vectores no son de soporte y así resolver un problema mas simple (ver, por ejemplo, el método de \emph{Sequential Minimal Optimization}). 


\begin{remark} Notemos que las características $\{x_i\}$ solo aparecen en forma de productos internos entre ellas mismas en la solución del método SVM, lo cual se puede ver desde la formulación de problema de optimización dual. Esto es clave para la extensión no lineal de SVM, pues no necesitamos operar directamente  con los valores de las entradas o características, sino con los productos punto entre ellas. En particular, si tenemos $N$ entradas de $M$ dimensiones, solo necesitamos los $N(N+1)/2$ productos puntos entre ellos y no todos los $NM$ valores, lo cual es particularmente relevante en caso que $M$ sea muy grande o incluso cuando $M=\infty$, i.e., las características $\{x_i\}$ son funciones.
\end{remark} 



\noindent\red{\textbf{Pendiente:} Interpretación física de SVM}


\subsection{Margen suave}

El planteamiento anterior tiene dos debilidades: la primera es que nuestros datos no siempre serán separables, y la segunda es que, incluso si los datos son linealmente separables, nuestro clasificador puede ser muy sensible a nuevos datos. Esto es ilustrado en la Fig.~\ref{fig:svm_softmargin}, donde el clasificador de máximo margen sin \emph{outlier} (izquierda) es muy distinto, o no-robusto, al obtenido en el caso de un \emph{outlier} (encerrado en púrpura). En general queremos que nuestro estimador sera robusto a este tipo de datos. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/cap5_margen_suave}
    \caption{Sensibilidad del máximo margen al agregar un nuevo dato (\emph{outlier}).}
    \label{fig:svm_softmargin}
\end{figure}

Para considerar que pueden existir datos que son posibles de clasificar con el clasificador lineal de máximo margen, podemos introducir las llamadas ``variables de holgura'' (\emph{slack variables}). Estas tienen el objetivo de que el clasificador admita algunos datos incorrectamente clasificados aún siendo óptimo. Específicamente, esto se logra reemplazando las restricciones para los datos mal clasificados (considerados \emph{outliers}) directamente en la formulación del problema de optimización, de esta forma, la \textit{mayoría} de los datos se clasifican de forma correcta con la finalidad de tener un modelo robusto.

La formulación del problema de clasificación que \emph{perdona} algunos datos mal clasificados mediante la utilización de variables de holgura $\{\xi_i\}_{i=1}^N$ es la siguiente:

\begin{equation*}
\begin{aligned}
& \underset{w,b, \xi}{\text{min}}
& & \frac{1}{2}||w||^2 + c\sum\limits_{i=1}^{N} \xi_i \\
& \text{s.a}
& & y_i (w\cdot x_i +b) \geq 1 - \xi_i, \; i = 1, \ldots, N,
\end{aligned}
\end{equation*}
donde $c>0$ es un hiper-parámetro. Observemos que la introducción del término $c\sum\limits_{i=1}^{N} \xi_i$ en la función de costo puede ser interpretada como una regularización, tal como lo hicimos con mínimos cuadrados. En efecto, veamos de las restricciones del problema anterior que los $\xi$ indican que tan mal clasificado está un punto, donde: 
\begin{itemize}
    \item si $\xi_i = 0$, entonces el dato $x_i$ está al lado correcto del plano y obtenemos el problema anterior
    \item si $0<\xi_i <1$, entonces $x_i$ está al lado correcto del plano, pero viola la restricción del margen, 
    \item si $\xi_i>1$, entonces el punto esta al lado incorrecto del plano.
\end{itemize}
Estas tres condiciones son ilustradas en la Fig.~\ref{fig:soft_margin}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{img/cap5_max_margen3}
    \caption{Valores para las variables de holgura $\xi$ en distintas regiones del espacio de entrada.}
    \label{fig:soft_margin}
\end{figure}

Procediendo de la misma forma que en el caso anterior, el dual de este problema es (lagrangiano):
\begin{equation*}
\begin{aligned}
& \underset{\alpha}{\text{max}}
& & \sum\limits_{i=1}^{N}\alpha_i - \frac{1}{2} \sum\limits_{i,j=1}^{N} \alpha_i \alpha_j y_i y_j \langle x_i, x_j\rangle\\
& \text{s.a}
& & \sum\limits_{i=1}^{N} \alpha_i y_i= 0 \\
& &  &0 \leq \alpha_i \leq c
\end{aligned}
\end{equation*}
Esta solución similar al caso anterior y también se puede resolver con técnicas de programación cuadrática. La diferencia entre ambas soluciones (con y sin variables de holgura) está dada por el hecho de que los multiplicadores de Lagrange ahora están acotados por el hiperparámetro $c$ que representa la importancia que se da a la sumatoria de las variables de holgura en la función de costo del problema de optimización.

Por último, ¿cómo definir el parámetro $c$? Es posible responder esta pregunta en relación al \textit{bias-variance tradeoff}. Notemos que como $\xi_i>1$ solo si la muestra $x_i$ está mal clasificada, entonces el término $\sum\limits_{i=1}^{N} \xi_i$ es una cota superior para la cantidad de muestras mál clasificadas. Consecuentemente, el hiperparámetro $c$ es un coeficiente (\emph{inverso}) de regularización, pues su magnitud controla el balance entre la maximización del margen y la cantidad muestras mal clasificadas. En particular, para un $c$ muy alto la cantidad de muestras mal clasificadas tiene mucho peso relativo comparada contra el ancho del margen en el funcional de minimización, con lo que la solución óptima tendrá un margen muy angosto y pocas (si es que hay alguna) muestras mal clasificadas. En efecto, si $c\to\infty$, se recupera la formulación del \emph{margen duro} y su misma solución de máximo margen (en el caso de que el problema sea efectivamente linealmente separable). Por el contrario, para un $c$ pequeño el margen tiene más importancia que la cantidad de datos incorrectamente clasificados, con lo que se encontrará un margen amplio con varias muestras mal clasificadas. Es de esperar que un $c$ pequeño (margen amplio) tenga mejor capacidad de generalizar a nuevos datos con menor varianza, mientras que un $c$ grande puede sobreajustar a los datos disponibles, reportando peor desempeño \emph{out-of-sample}. Por esto hemos dicho que $c$ es análogo a un coeficiente \emph{inverso} de regularización, pues mientras más pequeño, más regular es la solución. 

Al no conocer los estadísticos de los datos, en la práctica ajustamos el hiperparámetro $c$ mediante validación cruzada.

\subsection{Métodos de \emph{kernel}}

El clasificador de máximo margen (duro o blando) tiene una propiedad muy beneficiosas en el caso linealmente separable: la solución siempre existe y es única. Sin embargo, no tenemos ninguna garantía en los casos no separables, los cuales afloran casi siempre usualmente en la práctica. Consideremos el problema de clasificar una base de datos generadas con una función ``o exclusivo'', o ``XOR'', presentada en la Fig.~\ref{fig:xor}. Estos datos no son linealmente separable, con lo que al calcular el clasificador óptimo  de SVM no llegaríamos a ninguna solución coherente, sin embargo, notemos que es posible diseñar una características particulares para este conjunto donde sí son separables. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{img/cap5_xor}
    \caption{Datos XOR no son linealmente separables}
    \label{fig:xor}
\end{figure}

Consideremos el mapa desde  $\R^2$ a $\R^3$ definido mediante
\begin{equation}
    \phi: [x_1, x_2] \mapsto [x_1, x_2, x_1 x_2],
\end{equation}
y observemos que éste permite clasificar de forma lineal (y trivial) las clases del problema XOR: ambas quedan clasificadas mediante el plano $z=0$ en $\R^3$, esto es ilustrado en la Fig.~\ref{fig:xor_proyectado}. La función $\phi$ es un ejemplo de una ingeniería de características como las vistas en el capítulo de regresión no lineal, en este caso particular, la característica relevante era precisamente $x_1x_2$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{img/cap5_xor_3d_proyeccion}
    \caption{Los puntos rojos y azules corresponden a los datos mapeados a través de $\phi$. El plano $z=0$ es capaz de separar puntos rojos y puntos azules en este nuevo espacio.}
    \label{fig:xor_proyectado}
\end{figure}

%%VOLVER A PROYECTAR

\begin{remark}
En el caso del problema XOR conocíamos la característica apropiada, sin embargo, en el caso general no es claro cuál debe ser ``el buen $\phi$''. A pesar de esto, notemos que tanto para la formulación con margen duro o blando de SVM, la solución solo requerirá que seamos capaces de calcular los productos internos entre las características de cada entrada. Es decir, si consideramos un $\phi$ arbitrario para nuestro problema de clasificación, solo necesitaríamos calcular los productos puntos de la forma
\begin{equation}
    \langle \phi(x_i) , \phi(x_j) \rangle, 
\end{equation}
para todo $x_i,x_j$ en el conjunto de entrenamiento. 
\end{remark}

Podemos entonces aplicar la siguiente intuición: seguramente no podemos encontrar el $\phi$ exclusivo de cada problema, sino que uno (o una clase) que sea muy general, complejo, o ``universal'' y que nos sirva para una gran cantidad de problemas, tenemos la garantía de que lo podremos usar dentro de SVM si somos capaces de calcular su producto punto. Además, por ``general'' podemos entender ``de alta dimensión'': podríamos pensar que para un problema en el cual no sabemos el buen $\phi$, podemos considerar agregar y agregar características, con lo cual tendríamos una ``característica agregada'' de alta dimensión esperando que alguno de los términos agregados sea el que efectivamente separa las clases. Observemos que fue precisamente lo que hicimos en el caso del problema XOR, donde se agregó la característica $x_1x_2$. 

Para encontrar estos $\phi$ generales, veamos la siguiente definición
\begin{definition}[Mercer kernel]
    Un Mercer kernel es una función $K: X\times X \to \R$ tal que
\begin{itemize}
    \item Es simétrica $K(x_1 , x_2 ) = K (x_2 , x_1)$
    \item Es definida positiva, es decir
    $$\int_{X^2} K(x_1, x_2)g(x_1) g(x_2) dx_1 dx_2\geq 0$$
    para toda función $g:X\rightarrow\R$ continua. 
\end{itemize}

\end{definition}

\begin{lemma}
    Todo Mercer kernel se puede descomponer de la forma
\begin{equation}
    K(x_1, x_2) = \langle \phi(x_1) , \phi(x_2) \rangle
\end{equation}
donde $\phi: X \to \R^D$ y $D$ puede ser $\infty$.

\end{lemma}


\begin{remark}[El truco del kernel]
La introducción del concepto de kernel es fundamental en SVM. La definición y lema anteriores nos dicen que para cualquier función simétrica y definida positiva $K$, existe una función $\phi$ que puede ser incluso de dimensión infinita, tal que la evaluación del kernel en dos puntos cualquiera equivale al producto punto entre dos evaluaciones del función $\phi$ en los mismo puntos. Esto, sumado al hecho de que la solución de SVM solo requiere del cálculo de productos internos, nos permite construir \emph{kernel SVMs},  parametrizamos directamente el producto punto en el problema de optimización mediante el kernel, pues esto da la garantía que el mapa de características existe. El caso interesante es cuando ocupamos un kernel que corresponde a un $\phi$ infinito dimensional, pues estamos efectivamente realizando la clasificación en un espacio de dimensión infinita pero con un cálculo que solo requiere de una cantidad finita de cálculos, esto se llama \emph{el truco del kernel}, el cual puede aplicarse a cualquier algoritmo en donde las entradas solo aparezcan en la forma de productos punto, proceso que recibe el nombre de \emph{kernelización  }. 
\end{remark}


 Veamos distintos tipos de kernels y sus propiedades.

\begin{itemize}
    \item   \textbf{Kernel polinomial:}
    \begin{equation}
       K_{pol} (x, y) = (c + x^\top y)^d
    \end{equation}
    donde $x\geq 0$ es un parámetro libre y $d\in\N$ es el orden del polinomio. Para entender las propiedades del kernel polinomial, veamos que para $x,y\in\R^m$, $d=2$ obtenemos 
    \begin{align}
        K_{pol} (x, y)  &= \left(c + \sum_{i=1}^m x_iy_i\right)^2\\
                        &= \sum_{i=1}^m (x_i^2)(y_i^2) + \sum_{i=2}^m \sum_{j=1}^{i-1} (\sqrt{2}x_ix_j)(\sqrt{2}y_iy_j) + \sum_{i=1}^m (\sqrt{2c}x_i)(\sqrt{2c}y_i) + c^2\\
                        &=\langle \phi_{pol}(x) , \phi_{pol}(y) \rangle
    \end{align}
    donde 
    \begin{equation}
        \phi_{pol}(x) = [x_1^2,\ldots,x_m^2,\sqrt{2}x_1x_2,\ldots, \sqrt{2}x_{m}x_{m-1},\sqrt{2c}x_1,\ldots,\sqrt{2c}x_m,c]
    \end{equation}
    Es decir, al usar el kernel poliomial estamos implícitamente usando un mapa de características que contiene todos los monomios de grado hasta $d=2$ (si $c>0$) o bien todos los monomios de grado igual a $d=2$ (en caso que $c=0$). Esto se cumple para cualquier $d\in\N$.
    \item \textbf{Función de base radial\footnote{también conocido como kernel exponencial cuadrático.}:} Este kernel está definido por
    \begin{equation}
        K_{rbf} (x , y ) = \sigma^2 \exp\left(-\frac{(x -y)^2}{2l^2}\right)
    \end{equation}
    Las fronteras que entrega son suaves (infinitamente diferenciables) y tiene dos hiperparámetros: $\sigma^2$ representa la escala, y $l$ controla la oscilación de la curva. 
    
    \item \textbf{Kernel periódico:}
    \begin{equation}
       K_{per} (x , y) = \sigma^2 \exp\left(- \frac{2\sen^2 (\pi|x -y|/p)}{l^2}\right) 
    \end{equation}
    
    Este kernel es capaz de rescatar características periódicas en los datos (controlados por el parámetro $p$). Los otros parámetros cumplen la misma función que el kernel anterior. 

    
\end{itemize}

\subsection{\emph{Kernel ridge regression}}

Veamos que es directo kernelizar el método de mínimos cuadrados regularizados visto en la Sección \ref{sub:min_cuad_reg}. En particular, consideremos el caso de regularización cuadrática el cual, según vimos en la ec.~\eqref{eq:least_sq_soln}, tiene solución
\begin{equation}
    \theta_{MCR} = \left(\tX^\top\tX +\rho \eye\right)^{-1} \tX^\top Y  \label{eq:RR_soln1} 
\end{equation}
 y consecuentemente reporta una predicción en base a un nuevo input $x_\star$ dada por $\hat{y}_\star = \theta_{MCR}^\top x_\star$. Como podemos ver, la interacción de las entradas $\tX$ no aparece en la forma de productos internos, lo cual es necesario para la kernelización. Sin embargo, es posible re-escribir el parámetro óptimo usando la igualdad 
\begin{equation}
    (P^{-1} + B^\top R^{-1} B)^{-1}B^\top R^{-1}= P B^\top (BP B^\top + R)^{-1},\label{eq:matrix_inversion_lemma}
\end{equation}
e identificando $P^{-1}=\rho\eye$, $B=\tX$, $R=\eye$, para obtener el paámetro óptimo y la predicción respectivamente mediante:  
\begin{align}
   \theta_{MCR}     &= \tX^\top\left(\tX\tX^\top + \rho\eye\right) ^{-1} Y\label{eq:RR_soln2} \\
   \hat{y}_\star    &= \theta_{MCR}^\top x_\star = Y^\top \left(\tX\tX^\top + \rho\eye\right) ^{-1} \tX x_\star, \label{eq:RR_pred} 
\end{align}
donde los elementos $[\tX\tX^\top]_{ij} =\langle \tilde{x}_i,\tilde{x}_j \rangle$ y $[\tX x_\star]_i = \langle \tilde{x}_i,x_\star\rangle$  muestran que las entradas solo aparecen en la predicción en la forma de productos internos. Las próximas dos observaciones ilustran la importancia de esta formulación. 
\begin{remark}
Como $\tX\in\R^{N\times M}$, donde $N$ es la cantidad de datos y $M$ es la dimensión de los datos de entrada, entonces la matriz a invertir en la ec.~\eqref{eq:RR_soln1} es de tamaño $M\times M$, mientras que la matriz a invertir en la ec.~\eqref{eq:RR_soln2} es de tamaño $N\times N$. Como el costo de invertir una matriz es de orden cúbico (dependiendo del método) en su dimensión, será preferible utilizar una de las dos formulaciones en base a qué es mayor, la dimensión o el número de datos. 
\end{remark}

\begin{remark}
Como hemos descrito la predicción de MCR en la foma de mínimos cuadrados, podemos reemplazar las entradas $x$ en las descripción anterior por una característica $\phi(x)$, de dimensión $D$. Notemos que es posible incluso hacer esto cuando $D=\infty$, pues en cuyo caso si bien el parámetro tiene efectivamente dimensión $D=\infty$, no necesitamos utilizar el parámetro de forma independiente, sino que solo ``en producto punto'' con la nueva entrada para calcular la predicción, lo cual solo requiere una cantidad de cálculos del orden del número de muestras. 
\end{remark}

En efecto, reemplazando las entradas $\tilde{x}_i$ y $x_\star$ respectivamente por $\phi_i=\phi(x_i)$ y $\phi_\star = \phi(x_\star)$, y denotando el producto punto entre dos de estas características mediante la evaluación del kernel 
\begin{equation}
    K(x,x') = \langle \phi(x) , \phi(x') \rangle, 
\end{equation}
obtenemos $\hat{y}_\star    = Y^\top \left(K(\tX,\tX) + \rho\eye\right) ^{-1} K(\tX, x_\star)$, donde hemos abusado de notación a usar argumentos matriciales en el kernel. Esta predicción puede ser reordenada para dar 
\begin{equation}
   \hat{y}_\star    = \sum_{i=1}^N h_i K(x_i,x_\star)\label{eq:RR_pred2},
\end{equation}
donde hemos denotado el vector $h\in\R^N$ de la forma $h_i = [Y^\top \left(K(\tX,\tX) + \rho\eye\right) ^{-1}]_i$. Hay al tres observaciones muy relevantes que podemos hacer con respecto al resultado anterior. 

\begin{remark}[Similitud]
    Notemos de la ec.~\eqref{eq:RR_pred2} que si el kernel $K(x,x')$ es una medida de similitud entre $x$ y $x'$, entonces la predicción de $y_\star$ es una combinación lineal de los valores $h_i$ donde los pesos de cada valor son mayores a para las entradas $x_i$ que se parecen (con respecto a $K$) a la nueva entrada $x_\star$. A su vez, los valores $h_i$ son una transformación lineal de las entradas $Y$, con lo que la predicción de kernel RR combina linealmente salidas conocidas en base a cuán similar a los datos históricos es una nueva entrada. 
\end{remark}

\begin{remark}[Características infinito-dimensionales]
Como la expresión de la predicción en la ec.~\eqref{eq:RR_pred2} solo depende del kernel, no es necesario definir explícitamente el mapa de características $\phi$, sino que solo el kernel $K$. Esto abre la posibilidad de utilizar un kernel como el exponencial cuadrático presentado en la Subsección anterior, que corresponde a un $\phi$ de dimensión infinita.
\end{remark}


\begin{remark}[Modelo no paramétrico] 
Notemos que, al elegir un $\phi$ de dimensión infinita, entonces el parámetro $\theta$ también es de dimensión infinita, sin embargo, podemos interpretar el hecho de que solo necesitamos hacer un cálculo finito para representar $\hat{y}_\star$ como extraer la información suministrada por los datos que tenemos $\datos = \{x_1,\ldots,x_n\}$, los cuales son finitos (ver el Teorema del representante). Una consecuencia de esto es que la predicción en la ec.~\eqref{eq:RR_pred2} depende de todos los elementos de $\datos$, a diferencia de todos los modelos que hemos visto hasta ahora, donde la información de los datos (independiente de cuántos fueran) quedaba resumida en un parámetro de dimensión fija. Nos referimos a este tipo de modelo (con infinitos parámetros) como \emph{no paramétricos} debido a que su tratamiento no puede ser de la misma forma que los modelos con parámetros finitos.  
\end{remark}

\begin{remark}[Complejidad variable] 
Finalmente, el hecho que los modelos no paramétricos tengan una cantidad de términos creciente en la cantidad de observaciones implica que su complejidad y desempeño son crecientes también. Esto está de acuerdo con la intuición de que  un modelo no debería tener una complejidad o capacidad fija, sino que flexible a medida que se va considerado una mayo eficiencia.
\end{remark}

\noindent\red{\textbf{Pendiente:} rol de hiperparametros del kernel, demostración KRR}



\subsection{Kernel SVM}

Ahora volvemos a SVM para convertir un problema no linealmente separable mediante en uno linealmente separable (en algún espacio) mediante una \emph{kernelización}, es decir, reemplazando las entradas $x$ por un mapa de características $\phi(\cdot)$ de alta (o infinita) dimensión. Como vimos anteriormente, si la formulación de SVM está expresada en forma de productos internos (y sí que lo está), mediante el truco del kernel podemos parametrizar directamente dichos productos internos con un kernel $K(\cdot,\cdot)$ sin la necesidad de definir dicho explícitamente el mapa $\phi$. 

Reemplazando entonces las entadas por las características igual que en el caso de regresión de ridge tenemos que la kernelización del SVM (margen suave) tiene una formulación primal de acuerdo a 
\begin{equation*}
\begin{aligned}
& \underset{w,b}{\text{min}}
& & \frac{1}{2}||w||^2 + c\sum\limits_{i=1}^{N} \xi_i\\
& \text{s.a}
& & y_i (w\cdot \phi(x_i) +b) \geq 1- \xi_i, \; i = 1, \ldots, N.
\end{aligned}
\end{equation*}
Mientras que su formulación dual está dada por
\begin{equation*}
\begin{aligned}
& \underset{\alpha}{\text{max}}
& & \sum\limits_{i=1}^{N}\alpha_i - \frac{1}{2} \sum\limits_{i=1}^{N} \alpha_i \alpha_j y_i y_j \langle\phi(x_i), \phi(x_j)\rangle\\
& \text{s.a}
& & \sum\limits_{i=1}^{N} \alpha_i y_i= 0 \\
& &  &0 \leq \alpha_i \leq c.
\end{aligned}
\end{equation*}

Como hemos mencionado anteriormente, en la expresión anterior $\phi(x)$ solo aparece en formas de productos internos $\langle \phi(x_i), \phi(x_j)\rangle$. Consecuentemente, podemos ocupar el truco del kernel para diseñar directamente el producto punto mediante $K(x_i,x_j)$. Con esto, el problema de optimización en el dual se convierte en 
\begin{equation*}
\begin{aligned}
& \underset{\alpha}{\text{max}}
& & \sum\limits_{i=1}^{N}\alpha_i - \frac{1}{2} \sum\limits_{i=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i, x_j)\\
& \text{s.a}
& & \sum\limits_{i=1}^{N} \alpha_i y_i= 0 \\
& &  &0 \leq \alpha_i \leq c.
\end{aligned}
\end{equation*}

\begin{remark}[Kernel SVM]
Esta formulación, al igual que el SVM lineal original, es un QP  con solución única, pues  el kernel $K$ es definido positivo y por ende el funcional de optimización es cuadrático y convexo. Consecuentemente, una vez calculados todos los $N(N+1)/2$ productos de la forma $K(x_i, x_j)$, lo único que queda es resolver un problema QP.
\end{remark}

Ahora, simplemente eligiendo distintos kernels como los vistos anteriormente, podemos implementar el concepto anterior. La Fig.~\ref{fig:ksvm}  muestra esta implementación para dos kernels: a la izquierda se ocupó un kernel polinomial de grado $3$, dicho kernel permite un poco más de curvatura en los límites del clasificador, mientras que a la derecha se ocupó un kernel RBF, el cual es capaz de encontrar fronteras de decisión más curvadas (infinitamente diferenciables).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/cap5_svm_2kernels}
    \caption{Clasificación usando kernel SVM (margen suave) con distinto kernels: polinomial a la izquierda y RBF a la derecha.}
    \label{fig:ksvm}
\end{figure}


Finalmente, observemos que los métodos de kernel tienen una tremenda ventaja computacional con respecto a sus contrapartes sin kernel. Transforma los datos a un espacio de mayor dimensión explícitamente definiendo $\phi(x)$ puede ser muy costoso, o incluso imposible en el caso que el espacio de llegada sea infinitamente dimensional. Usando funciones kernel, por el contrario, dicha función queda implícita en el problema, dejándonos incluso trabajar en el caso infinito-dimensional pero a costo finito (en el caso de RBF por ejemplo).


%-Comparar rendimiento de logistic regression c on SVM lineal
%-Usar diferentes Kernels
%-Ver el overfitting cuando ocupas un polinomio de muchos grados
%-Ver la importancia de escalar los datos


